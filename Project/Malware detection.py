#!/usr/bin/env python
# coding: utf-8

# ### project- mlaware detection

# ### importing libraries

# In[1]:


#Importing the required libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier


# ### importing the dataset

# In[3]:


#Importing the dataset

df = pd.read_csv(r'C:\Users\resmi\Downloads\dataset.csv')

#Displaying first five rows of the dataset
df.head()


# ### shape of the datset

# In[4]:


#Using pandas "shape" method to check the dimensionality of the dataset

df.shape


# ### column names

# In[5]:


df.columns


# ### Checking data type of all columns

# In[6]:


#Using pandas "dtypes" method to check the data type of all columns in the dataset

df.dtypes


# ### Checking Missing Values

# In[7]:


#Using pandas "isnull" method to check missing values in the dataset

df.isnull().sum()


# ### Descriptive Analysis

# In[8]:


#Using pandas "describe" function to get the statistical summary of the dataset

df.describe()


# ### Correlation

# In[9]:


#Using pandas "corr" method to check the correlation among different variables in the dataset

dfcorr = df.corr()
dfcorr


# ### Correlation Visualization

# In[10]:


#Using seaborn's "heatmap" to visualize the correlation among variables 

plt.figure(figsize=(22,14))
sns.heatmap(dfcorr, annot = True, cmap = "Greens")


# ### Dropping columns which do not have any impact on the classification column and are not correlated with each other

# ### Checking the correlation again on new and cleaned data

# In[19]:


df = df.drop(['hash',"usage_counter","static_prio","normal_prio","policy","vm_pgoff","task_size","cached_hole_size","hiwater_rss","nr_ptes","lock","cgtime","signal_nvcsw","state","free_area_cache","min_flt","fs_excl_counter"],axis=1)


# In[20]:


dfcorr1 = df.corr()
plt.figure(figsize=(22,14))
sns.heatmap(dfcorr1, annot = True, cmap = "YlOrRd")


# In[21]:


df.shape


# ### Data Preprocessing

# In[22]:


#Defining the variables

#defining "x" or independent variables in the dataset

x = df.drop("classification", axis = 1).to_numpy()  

#placing target variable "classification" in y variable

y = df["classification"].to_numpy() 


# In[23]:


x_train,x_test,y_train,y_test=train_test_split(x,y,stratify=y, test_size=0.2,random_state=100)  

print("number of test samples :", x_test.shape[0])
print("number of training samples:",x_train.shape[0])


# In[24]:


sc = StandardScaler()
x_train2 = sc.fit_transform(x_train)
x_test2 = sc.transform(x_test)


# ### Model Building

# ### Support Vector Machines and Naive Bayes

# In[25]:


for name,method in [('SVM', SVC(kernel='linear',random_state=100)),
                    ('Naive Bayes',GaussianNB())]: 
    method.fit(x_train2,y_train)
    predict = method.predict(x_test2)
    print('\nEstimator: {}'.format(name)) 
    print(confusion_matrix(y_test,predict))  
    print(classification_report(y_test,predict))


# ### Decision Tree

# In[26]:


for name,method in [('DT', DecisionTreeClassifier(random_state=100))]:
    method.fit(x_train2,y_train)
    predict = method.predict(x_test2)
    print('\nEstimator: {}'.format(name))
    print(confusion_matrix(y_test,predict))
    print(classification_report(y_test,predict)) 


# In[ ]:




